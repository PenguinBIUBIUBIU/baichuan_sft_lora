import json
import torch
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import BitsAndBytesConfig, GenerationConfig
from peft import PeftModel

import sys
if len(sys.argv) >= 4:
    MODEL_NAME_OR_PATH = sys.argv[1]
    LORA_WEIGHTS_PATH = sys.argv[3]
else:
    print("Usage: python chat__web_demo.py [MODEL_NAME_OR_PATH] [LORA_WEIGHTS_PATH]")
    sys.exit(1)

st.set_page_config(
    page_title="baichuan-7B-int4 æ¼”ç¤º",
    page_icon=":robot:",
    layout='wide'
)
st.title("baichuan-7B-int4")

max_new_tokens = st.sidebar.slider(
    'max_length', 0, 32768, 1024, step=1
)
top_p = st.sidebar.slider(
    'top_p', 0.0, 1.0, 0.8, step=0.01
)
temperature = st.sidebar.slider(
    'temperature', 0.0, 1.0, 0.8, step=0.01
)

@st.cache_resource
def init_model():
    ###åŠ è½½é‡åŒ–æ¨¡å‹
    device_map = {"": 0}
    print(f"{MODEL_NAME_OR_PATH}:åŠ è½½æ¨¡å‹")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME_OR_PATH,
                                                 trust_remote_code=True,
                                                 quantization_config=BitsAndBytesConfig(
                                                     load_in_4bit=True,
                                                     bnb_4bit_compute_dtype=torch.bfloat16,
                                                     bnb_4bit_use_double_quant=True,
                                                     bnb_4bit_quant_type='nf4'
                                                 ),
                                                 device_map=device_map)

    generation_config = GenerationConfig(
        temperature=temperature,
        top_p=top_p,
        do_sample=True,
        repetition_penalty=2.0,
        max_new_tokens=max_new_tokens,  # max_length=max_new_tokens+input_sequence
    )
    model.generation_config = generation_config

    ###ç»„è£…lora
    device = "cuda:0"
    print(f"{LORA_WEIGHTS_PATH}:PEFTåŠ è½½æ¨¡å‹")
    model_lora = PeftModel.from_pretrained(
        model,
        LORA_WEIGHTS_PATH
    ).to(device)
    model_lora = model_lora.eval()
    return tokenizer, model_lora

tokenizer, model = init_model()

def clear_chat_history():
    del st.session_state.messages

def init_chat_history():
    with st.chat_message("assistant", avatar='ğŸ¤–'):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾å·å¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

    if "messages" in st.session_state:
        for message in st.session_state.messages:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ¤–'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages = []

    return st.session_state.messages

def main():
    model, tokenizer = init_model()
    messages = init_chat_history()

    if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
        with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
            st.markdown(prompt)
        messages.append({"role": "user", "content": prompt})
        print(f"[user] {prompt}", flush=True)
        with st.chat_message("assistant", avatar='ğŸ¤–'):
            placeholder = st.empty()
            for response in model.chat(tokenizer, messages, stream=True):
                placeholder.markdown(response)
                if torch.backends.mps.is_available():
                    torch.mps.empty_cache()
        messages.append({"role": "assistant", "content": response})
        print(json.dumps(messages, ensure_ascii=False), flush=True)

        st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)

if __name__ == "__main__":
    main()